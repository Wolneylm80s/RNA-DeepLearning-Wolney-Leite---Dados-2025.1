{"cells":[{"cell_type":"markdown","metadata":{"id":"HnkA6fazG2Eh"},"source":["# PyTorch: Variable, Gradientes e Grafo Computacional"]},{"cell_type":"markdown","metadata":{"id":"3934c097"},"source":["## Atualizando para usar Tensores com `requires_grad=True`\n","\n","A classe `Variable` foi descontinuada no PyTorch. Agora, a funcionalidade de rastreamento de gradiente est√° integrada diretamente aos tensores. Para habilitar o c√°lculo autom√°tico de gradientes para um tensor, defina o par√¢metro `requires_grad` como `True` durante a cria√ß√£o do tensor ou use o m√©todo `.requires_grad_(True)`."]},{"cell_type":"markdown","metadata":{"id":"SdiT8nGeG2Ek"},"source":["## Objetivos"]},{"cell_type":"markdown","metadata":{"id":"eDEX9Mk8G2Em"},"source":["Este notebook introduz\n","- o conceito de `Variables` do PyTorch,\n","- uma interpreta√ß√£o num√©rica intuitiva do gradiente, e o\n","- grafo computacional, utilizado para o c√°lculo autom√°tico do gradiente de uma fun√ß√£o."]},{"cell_type":"markdown","metadata":{"id":"3lPCM5vmG2Eo"},"source":["Um dos principais fundamentos para que o PyTorch seja adequado para deep learning √© a sua habilidade de\n","calcular o gradiente automaticamente a partir da express√µes definidas. Essa facilidade √© implementada\n","pelo tipo Variable do PyTorch, que adiciona ao tensor a facilidade de c√°lculo autom√°tico do gradiente pela constru√ß√£o din√¢mica do grafo computacional."]},{"cell_type":"markdown","metadata":{"id":"RLATrVDrG2Eq"},"source":["## Grafo computacional"]},{"cell_type":"markdown","metadata":{"id":"VbH9rnZmG2Er"},"source":["```\n","    y_pred = x * w\n","    e = y_pred - y\n","    e2 = e**2\n","    J = e2.sum()\n","```"]},{"cell_type":"markdown","metadata":{"id":"6tPYkCldG2Es"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/GrafoComputacional.png)"]},{"cell_type":"markdown","metadata":{"id":"MnTsPEf6G2Eu"},"source":["Variable possui 3 campos: o dado em si (data), o gradiente (grad) e um apontador (creator) para construir o grafo da backpropagation. Uma express√£o utilizada para o c√°lculo do gradiente exige que todas suas express√µes sejam calculadas com Variables, caso contr√°rio n√£o √© poss√≠vel construir o grafo computacional."]},{"cell_type":"markdown","metadata":{"id":"1wI-LZc6G2Ev"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/variables.png)"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.148492","start_time":"2017-11-24T14:09:18.096752"},"id":"OJt_zc0CG2Ex","executionInfo":{"status":"ok","timestamp":1762991406355,"user_tz":180,"elapsed":5492,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[],"source":["import torch\n","# from torch.autograd import Variable # Variable is deprecated"]},{"cell_type":"markdown","metadata":{"id":"JONBAEwzG2E4"},"source":["## Variable √© criada a partir de um tensor e possui as mesmas funcionalidades"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.305082","start_time":"2017-11-24T14:09:19.150032"},"colab":{"base_uri":"https://localhost:8080/"},"id":"aT8CBLTaG2E6","outputId":"3f92cb76-eba8-4bf9-a944-6e789dc17c61","executionInfo":{"status":"ok","timestamp":1762991406419,"user_tz":180,"elapsed":60,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 2., 4., 6.])"]},"metadata":{},"execution_count":2}],"source":["y_t = 2 * torch.arange(0.,4.)\n","y = y_t; y # Use tensor directly"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.311537","start_time":"2017-11-24T14:09:19.306712"},"colab":{"base_uri":"https://localhost:8080/"},"id":"WiSNffiPG2FD","outputId":"25bc3b6a-2a50-4408-a89b-44140a2aa538","executionInfo":{"status":"ok","timestamp":1762991406425,"user_tz":180,"elapsed":4,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 2., 3.])"]},"metadata":{},"execution_count":3}],"source":["x = torch.arange(0.,4.); x # Use tensor directly"]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.318102","start_time":"2017-11-24T14:09:19.313131"},"colab":{"base_uri":"https://localhost:8080/"},"id":"dW0pCsgaG2FK","outputId":"16c3ab3b-3427-413a-d52b-1d5629c84cf4","executionInfo":{"status":"ok","timestamp":1762991406435,"user_tz":180,"elapsed":9,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.], requires_grad=True)"]},"metadata":{},"execution_count":4}],"source":["w = torch.ones(1, requires_grad=True); w # Use tensor with requires_grad=True"]},{"cell_type":"markdown","metadata":{"id":"VRXfFM2TG2FR"},"source":["## C√°lculo autom√°tico do gradiente da fun√ß√£o perda J"]},{"cell_type":"markdown","metadata":{"id":"tFuPZhFGG2FU"},"source":["Seja a express√£o: $$ J = ((x  w) - y)^2 $$\n","\n","Queremos calcular a derivada de $J$ em rela√ß√£o a $w$."]},{"cell_type":"markdown","metadata":{"id":"KqNof23cG2FW"},"source":["### Montagem do grafo computacional"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.344631","start_time":"2017-11-24T14:09:19.319779"},"colab":{"base_uri":"https://localhost:8080/"},"id":"71lXNZ9mG2FX","outputId":"39c303f7-9260-4041-9c9f-4aa565aad332","executionInfo":{"status":"ok","timestamp":1762991406522,"user_tz":180,"elapsed":14,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14., grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":5}],"source":["# predict (forward)\n","y_pred = x * w\n","\n","# c√°lculo da perda J: loss\n","e = y_pred - y\n","e2 = e.pow(2)\n","J = e2.sum()\n","J"]},{"cell_type":"markdown","metadata":{"id":"elNRuA34G2Fe"},"source":["## Auto grad - processa o grafo computacional backwards"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2017-10-04T15:55:45.308858","start_time":"2017-10-04T15:55:45.304654"},"id":"y9mN5TqhG2Fm"},"source":["O `backward()` varre o grafo computacional a partir da vari√°vel a ele associada e calcula o gradiente para todas as `Variables` que possuem o atributo `requires_grad` como verdadeiro.\n","O `backward()` destroi o grafo ap√≥s sua execu√ß√£o. Isso √© intr√≠nsico ao PyTorch pelo fato dele ser uma rede din√¢mica."]},{"cell_type":"code","execution_count":6,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.355085","start_time":"2017-11-24T14:09:19.346403"},"colab":{"base_uri":"https://localhost:8080/"},"id":"DrEXyjIDG2Fn","outputId":"f0dbebed-5cdb-4905-b3f7-69c4355f29a4","executionInfo":{"status":"ok","timestamp":1762991406589,"user_tz":180,"elapsed":66,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-28.])\n"]}],"source":["J.backward()\n","print(w.grad)"]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.363848","start_time":"2017-11-24T14:09:19.356800"},"id":"MwkCb8LYG2Fu","executionInfo":{"status":"ok","timestamp":1762991406594,"user_tz":180,"elapsed":3,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[],"source":["w.grad.data.zero_();"]},{"cell_type":"markdown","metadata":{"id":"9biEyEt1G2Fz"},"source":["## Interpreta√ß√£o do Gradiente"]},{"cell_type":"markdown","metadata":{"id":"KVaH2WFyG2F0"},"source":["O gradiente de uma vari√°vel final (J) com respeito √† outra vari√°vel de entrada (w) pode ser interpretado como o quanto a vari√°vel final J vai aumentar se houver um pequeno aumento na vari√°vel de entrada (w).\n","Por exemplo suponha que o gradiente seja 28. Isto significa se aumentarmos a vari√°vel w de 0.001, ent√£o J vai aumentar de 0.028."]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.372040","start_time":"2017-11-24T14:09:19.365927"},"colab":{"base_uri":"https://localhost:8080/"},"id":"RnO_ImuHG2F2","outputId":"810ca7f0-e672-4acb-d2ab-c4692309e24a","executionInfo":{"status":"ok","timestamp":1762991406685,"user_tz":180,"elapsed":16,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(13.9720, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":8}],"source":["eps = 0.001\n","y_pred = x * (w+eps)\n","J_new = (y_pred - y).pow(2).sum()\n","J_new"]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.403233","start_time":"2017-11-24T14:09:19.373831"},"colab":{"base_uri":"https://localhost:8080/"},"id":"1ObOY_GnG2F8","outputId":"5f544280-eb87-4d53-af7b-f5e0b80f295e","executionInfo":{"status":"ok","timestamp":1762991406747,"user_tz":180,"elapsed":60,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-0.027988434\n"]}],"source":["print((J_new - J).data.numpy())"]},{"cell_type":"markdown","metadata":{"id":"v625mnGlG2GC"},"source":["## Back propagation"]},{"cell_type":"markdown","metadata":{"id":"ryb4BtcGG2GF"},"source":["Uma forma equivalente expl√≠cita de calcular o gradiente √© fazendo o processamento do backpropagation no grafo computacional, de forma expl√≠cita.\n","Apenas como ilustra√ß√£o.\n","\n","Fun√ß√£o de perda:\n","$$ J(\\hat{y_i},y_i) = \\frac{1}{M} \\sum_{i=0}^{M-1} (\\hat{y_i} - y_i)^2 $$\n","\n","Gradiente:\n","$$  \\mathbf{\\nabla{J_w}} = \\frac{2}{M}\\mathbf{x^T}(\\mathbf{x w^T} - \\mathbf{y}) $$\n","\n","Atualiza√ß√£o dos par√¢metros pelo gradiente descendente:\n","$$ \\mathbf{w} = \\mathbf{w} ‚àí \\eta (\\mathbf{\\nabla J_w})^T $$"]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2017-11-24T14:09:19.413681","start_time":"2017-11-24T14:09:19.405014"},"colab":{"base_uri":"https://localhost:8080/"},"id":"HXrz6a0EG2GH","outputId":"bbde03e8-c448-49e7-af53-1169005de33a","scrolled":false,"executionInfo":{"status":"ok","timestamp":1762991406754,"user_tz":180,"elapsed":5,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n","[1. 1. 1. 1.]\n","[ 0. -2. -4. -6.]\n","-28.0\n"]}],"source":["import numpy as np\n","\n","dJ = 1.\n","de2 = dJ * np.ones((4,))\n","de = de2 * 2 * e.data.numpy()\n","dy_pred = de\n","dw = (dy_pred * x.data.numpy()).sum()\n","print(dJ)\n","print(de2)\n","print(de)\n","print(dw)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"bIqUBct4G2Gg"},"source":["# Exerc√≠cios"]},{"cell_type":"markdown","metadata":{"id":"vEfWG-e5G2Gh"},"source":["## Quest√µes"]},{"cell_type":"markdown","metadata":{"id":"PXLoZzF6G2Gj"},"source":["1. O que acontece com o grafo computacional ap√≥s execu√ß√£o do `backward()`?\n",">  Ap√≥s executar o PyTorch percorre todo o grafo computacional de tr√°s para frente, acumulando em cada tensor. Depois de calcular todos os gradientes, o PyTorch descarta o grafo, liberando a mem√≥ria."]},{"cell_type":"markdown","metadata":{"id":"ayWukpAIG2Gk"},"source":["## Atividades"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2017-10-06T22:02:17.474843Z","start_time":"2017-10-06T22:02:17.468865Z"},"id":"k5Ayt5dAG2Gl"},"source":["1. Execute um passo de atualiza√ß√£o do valor de w, pelo\n","gradiente descendente. Utilize um fator de aprendizado (*learning rate*) de 0.1\n","para atualizar o `w`. Ap√≥s, recalcule a fun√ß√£o de perda:\n","\n","    - w = w - lr * w.grad.data\n","    - execute a c√©lula 1.3.1 e verifique o quanto que a perda J diminuiu\n","    "]},{"cell_type":"code","source":["# Passo 1: atualizar w\n","lr = 0.1\n","w.data = w.data - lr * w.grad.data\n","\n","# Passo 2: zerar gradiente\n","w.grad.data.zero_()\n","\n","# Passo 3: recalcular J\n","y_pred = x * w\n","J = (y_pred - y).pow(2).sum()\n","print(\"Nova Loss J:\", J.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxi7Wks8uBJq","executionInfo":{"status":"ok","timestamp":1762991406760,"user_tz":180,"elapsed":5,"user":{"displayName":"WOLNEY LEITE MIRANDA","userId":"18002588653260925306"}},"outputId":"1b408d60-244c-4b03-dbb7-d40bab540189"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Nova Loss J: 14.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"7S6srVMzG2G1"},"source":["# Aprendizados com este notebook"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"VyUo4FaAG2G2"},"source":["1. Tensors com requires_grad=True\n","\n","O PyTorch utiliza tensores que armazenam n√£o apenas valores num√©ricos, mas tamb√©m gradientes.\n","\n","Quando requires_grad=True, o PyTorch registra as opera√ß√µes para construir um grafo computacional.\n","\n","2. Grafo computacional\n","\n","Cada opera√ß√£o matem√°tica cria n√≥s no grafo.\n","\n","Esse grafo representa a cadeia de opera√ß√µes necess√°rias para calcular o gradiente da fun√ß√£o final.\n","\n","Ap√≥s chamar backward(), o grafo √© destru√≠do para economizar mem√≥ria.\n","\n","3. Gradiente num√©rico\n","\n","Utilizando pequenas perturba√ß√µes Œµ √© poss√≠vel aproximar o gradiente numericamente:\n","\n","ùëì( ùë§ + ùúÄ)\n","‚àí\n","ùëì\n","(\n","ùë§\n",")\n","ùúÄ\n","Œµ\n","f(w+Œµ)‚àíf(w)\n","\t‚Äã\n","\n","\n","Isso serve para verificar se o gradiente anal√≠tico/autograd est√° correto.\n","\n","4. Autograd e backward()\n","\n","O PyTorch calcula automaticamente os gradientes usando backpropagation.\n","\n","O gradiente aparece em w.grad.\n","\n","Os gradientes s√£o acumulados, por isso devem ser zerados antes de novo passo.\n","\n","5. Gradiente descendente manual\n","\n","A atualiza√ß√£o de pesos pode ser feita manualmente:\n","\n","ùë§\n",":\n","=\n","ùë§\n","‚àí\n","ùúÇ\n","‚ãÖ\n","‚àÇ\n","ùêΩ\n","‚àÇ\n","ùë§\n","w:=w‚àíŒ∑‚ãÖ\n","‚àÇw\n","‚àÇJ\n","\t‚Äã\n","\n","\n","Isso permite aprender como frameworks funcionam internamente, antes de usar otimizadores mais avan√ßados."]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/vcasadei/Redes-Neurais-CESAR-School/blob/2025.01/2%20-%20Regress%C3%A3o%20Linear/2_4_PyTorch_Variaveis_Gradientes.ipynb","timestamp":1762128440347}]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.2"},"vscode":{"interpreter":{"hash":"5fe3e6f0cdaab8afdc61c52912fda83f7c0a71baaea1897dd7498e2df01e69ec"}}},"nbformat":4,"nbformat_minor":0}